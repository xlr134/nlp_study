ELMo（Embeddings from Language Models）
结构特点：它是基于双向 LSTM（Long - Short - Term Memory）构建的。通过对大量文本的正向和反向语言模型训练，
能够学习到单词在不同语境下的复杂语义表示。对于一个给定的单词，它会根据其所在句子的上下文生成不同的词向量。
例如，“bank” 这个词在 “我去银行存钱” 和 “河岸边长满了青草” 这两个句子中会有不同的语义表示，ELMo 能够很好地捕捉这种差异。
GPT（Generative Pretrained Transformer）
结构特点：采用 Transformer 架构的解码器部分。它是一个自回归语言模型，通过预测下一个单词的概率分布来生成
文本。以生成新闻文章为例，给定文章的开头部分，GPT 可以根据训练学到的语言模式和语义知识，逐词生成后续的内
容。它的 Transformer 解码器能够处理长序列数据，并且通过多头注意力机制（Multi - Head Attention）来综合考虑文本中的不同语义信息。
BERT（Bidirectional Encoder Representations from Transformers）
结构特点：基于 Transformer 架构的编码器部分。其主要特点是采用了双向训练的方式，能够同时学习单词的左右两
侧上下文信息。在进行文本分类任务时，例如判断一段电影评论是正面还是负面，BERT 可以通过对整个评论句子进行
双向编码，充分理解句子的语义和情感倾向。它还引入了 Masked Language Modeling（MLM）和 Next Sentence 
rediction（NSP）任务来进行预训练，增强了模型对文本语义和句子关系的理解。
Ernie - baidu（Enhanced Representation through kNowledge IntEgration）
结构特点：在 Transformer 架构基础上，融合了知识图谱中的知识。它能够将文本中的实体与知识图谱中的实体进行
关联，从而更好地理解文本的语义。比如在处理包含历史人物名字的文本时，它可以利用知识图谱中关于该人物的出生
年代、主要事迹等知识来更准确地理解和生成相关文本。
Ernie - Tsinghua（Enhanced Representation through kNowledge IntEgration）
结构特点：和百度的 Ernie 类似，也注重知识融合。它利用知识增强的预训练策略，通过 Transformer 架构对文
本和知识进行联合建模。在处理学术文献等知识密集型文本时，可以利用学术知识图谱等外部知识来提升对文本的理解和生成能力。
GPT2
结构特点：是 GPT 的升级版，同样基于 Transformer 解码器架构。它具有更大的模型参数和更强大的生成能力。可
以生成连贯、高质量的长文本，如小说、故事等。其结构上通过增加网络层数、头数等方式来提升模型的表达能力，并
且在预训练过程中使用了更多的文本数据，能够更好地学习语言的统计规律和语义信息。
UNILM（Unified Language Model）
结构特点：采用 Transformer 架构，并且能够同时进行自然语言理解和生成任务。它通过巧妙的掩码机制，在不同的
任务中灵活地控制语言模型的输入和输出。例如，在文本生成任务中，它可以像 GPT 一样生成文本；在文本分类任务
中，它又可以像 BERT 一样对文本进行编码理解，实现了多种自然语言处理任务的统一建模。
Transformer - XL & XLNet
Transformer - XL 结构特点：它在 Transformer 架构基础上引入了循环机制，能够处理更长的文本序列。它通过缓
存之前层的隐藏状态，使得模型在处理长文本时能够更好地利用历史信息。例如在处理长篇小说等长文本时，可以避免
信息丢失。
XLNet 结构特点：是一种自回归语言模型，基于 Transformer - XL 架构构建。它采用了一种新的排列语言建模（Per
mutation Language Modeling）方法，能够学习单词之间的各种顺序关系。与传统的自回归模型不同，它不是简单地
按照从左到右或从右到左的顺序预测单词，而是考虑了单词的所有可能排列顺序，从而更好地捕捉文本的语义结构。
Roberta（A Robustly Optimized BERT Pretraining Approach）
结构特点：本质上是对 BERT 的优化。它使用了更多的训练数据和更优化的训练策略。在预训练过程中，对 BERT 
的训练任务进行了改进，例如调整了 Masked Language Modeling 的方式，使得模型能够更好地学习文本的语义和
语法规则。在处理语义相似性判断等任务时，能够提供更准确的文本表示。
SpanBert
结构特点：也是基于 Transformer 架构的模型。它着重改进了对文本跨度（span）的表示学习。通过新的训练任务
，如跨度预测任务，能够更好地理解文本中连续的单词序列，如短语、命名实体等。在信息提取任务中，比如从新闻
报道中提取公司名称和相关事件等，SpanBert 可以更有效地利用文本跨度信息来提高提取的准确性。
ALBERT（A Lite BERT）
结构特点：对 BERT 进行了参数精简和结构优化。它通过参数共享等方式减少了模型的参数数量，同时保持了较好的
性能。在处理大规模文本数据时，能够以较低的计算成本实现高效的文本理解和生成任务。例如，在资源受限的设备上
，ALBERT 可以在保证一定准确性的前提下，更快地完成文本分类等任务。
InstructGPT
结构特点：基于 GPT 架构，重点在于能够更好地遵循人类的指令。通过在预训练过程中结合人类的反馈，如人工标注的
指令 - 响应对，来调整模型的输出，使得模型生成的文本更符合人类的期望。在问答系统等应用场景中，可以根据用户
的提问指令生成更准确、更有用的回答。
GPT3
结构特点：是一个非常大规模的 Transformer 解码器架构模型。具有海量的参数，能够生成高质量、多样化的文本。它
可以在几乎没有特定任务训练的情况下，通过少量示例（few - shot learning）或无示例（zero - shot learning）
来完成多种语言任务，如翻译、摘要、问答等。其巨大的模型规模使得它能够学习到非常复杂的语言模式和语义关系。
