# -*- coding: utf-8 -*-
"""开源大模型结构对比.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ggAt6Rmx_Ghq089pSSYGwVXrLmJx5AAy

记录大模型的结构差异以及代码解读，每个大模型都配有详细的说明各个部分在模型的什么位置，是如何配置的

| 模型           | 位置编码  | Transformer结构 | 多头机制    | FF层设计     | 归一化层选择         | 激活函数 | 是否使用bias            |
|----------------|-----------|-----------------|-------------|--------------|----------------------|----------|-------------------------|
| baichuan2-7b   | RoPE      | 串行            | 传统方式    | gated形式    | RMSnorm/pre norm     | SiLU     | 无bias                 |
| baichuan2-13b  | Alibi     | 串行            | 传统方式    | gated形式    | RMSnorm/pre norm     | SiLU     | 无bias                 |
| chatglm2       | RoPE      | 串行            | multi query | gated形式    | RMSnorm/pre norm     | SiLU     | qkv有bias，其他线性层无bias |
| llama2         | RoPE      | 串行            | multi query | gated形式    | RMSnorm/pre norm     | SiLU     | 无bias                 |
| moss           | RoPE      | 平行            | 传统方式    | 传统方式     | LayerNorm            | gelu_new | sa无bias, ff有bias     |
| qwen           | RoPE      | 串行            | 传统方式    | gated形式    | RMSnorm/pre norm     | SiLU     | qkv有bias，其他线性层无bias |
| grok           | RoPE      | 串行            | group query(默认不启用)    | moe形式    | RMSnorm/sandwich norm     | gelu     | 无bias |
| mixtral           | RoPE      | 串行            | group query    | moe形式     | RMSNorm/pre norm     | silu | 无bias |
| drbx           | RoPE      | 串行            | multi query    | moe形式     | LN/pre norm     | silu | 无bias |
| gemma           | RoPE      | 串行            | group query(默认不启用)    | gated形式     | RMSnorm/pre norm     | gelu | 无bias |
| deepseek v3           | RoPE      | 串行            | MLA    | 共享moe     | RMSnorm/pre norm     | silu | 无bias |

## grok

# 模型入口class Grok1Model(Grok1PretrainedModel)

依次通过



1.   Embedding
1.   32层Decoder
2.   RMSNorm

# class DecoderLayer(nn.Module):

串行结构 Sandwich-LN

x -> pre_attn_norm -> self_attn -> post_attn_norm -> 残差链接x

x -> pre_moe_norm -> moe -> post_moe_norm -> 残差链接x -> 输出

# 注意力层class MultiHeadAttention(nn.Module)

使用RoPE，可配置group query attention，但默认配置下q和k,v头相同

    if num_key_value_heads is None:
        num_key_value_heads = num_heads
    self.num_key_value_heads = num_key_value_heads
    self.num_key_value_groups = self.num_heads // self.num_key_value_heads


q,k,v无bias

    self.q_proj = nn.Linear(hidden_size, self.num_heads * self.head_dim, bias=False)
    self.k_proj = nn.Linear(hidden_size, self.num_key_value_heads * self.head_dim, bias=False)
    self.v_proj = nn.Linear(hidden_size, self.num_key_value_heads * self.head_dim, bias=False)
    self.o_proj = nn.Linear(self.num_heads * self.head_dim, hidden_size, bias=False)

# Moe层class MoeBlock(nn.Module)

top_k = 2
先过一个router线性层，得出每个token该去哪topk个专家层

    self.gate = nn.Linear(hidden_dim, num_experts, bias=False)

构建一个专家层的mask，遍历专家层，由于mask的作用，只有选择了当前专家的token才会参与运算，每次专家的结果都加到final_state上

# Moe里的专家层class MoeMLP(nn.Module)

gate结构

    self.linear_v = nn.Linear(hidden_dim, ffn_dim, bias=False)
    self.linear_1 = nn.Linear(ffn_dim, hidden_dim, bias=False)
    self.linear = nn.Linear(hidden_dim, ffn_dim, bias=False)
    self.act_fn = nn.GELU()
    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
        current_hidden_states = self.act_fn(self.linear(hidden_states)) * self.linear_v(hidden_states) # 从hidden_dim变到ffn_dim
        current_hidden_states = self.linear_1(current_hidden_states) # 再变回来
        return current_hidden_states

---------------------------------------------------------------------------------------------------------------------------------------------------------------

## Mixtral

# 模型入口class MixtralModel(MixtralPreTrainedModel)

依次通过


1.   Embedding
1.   32层MixtralDecoderLayer
2.   MixtralRMSNorm

# transformer块class MixtralDecoderLayer(nn.Module)

串行结构 Pre Norm
x -> RMSNorm -> attn -> 残差链接 -> moe -> 残差链接

# 注意力层class MixtralAttention(nn.Module)

使用group query的注意力机制，Q 32 个头，KV 8 个头

使用RoPE位置编码

# Moe层class MixtralSparseMoeBlock(nn.Module)

一个Linear层gate，选出top2个专家层
for循环所有专家层，计算选了这个专家的token的计算结果，再取概率作为权重加到结果上
简言之，假设 token0 选的是专家0，概率0.6，和专家4概率0.4，那最后结果token0对应的就是 0.6*专家0(token0) + 0.4*专家4(token0)

# 专家层 class MixtralBlockSparseTop2MLP(nn.Module)

gate结构，先放大到14336，再缩回4096

-------------------------------------------------------------------------------------------

## DBRX

# 模型主体 class DbrxModel(DbrxPreTrainedModel)

x -> Embedding -> 40层Transformer -> Norm

# transformer层 class DbrxBlock(nn.Module)

pre Norm
x -> Norm-Attn-Norm -> 残差 + hidden_state -> ffn -> 残差链接

# attention层 class DbrxNormAttentionNorm(nn.Module)

LN RoPE
Multi-query head, 因为默认配置中 kv头只有1个

扩展kv头的方法, 供参考

    def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:
        '''Equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep).
        The hidden states go from (batch, num_key_value_heads, seqlen, head_dim) to
        (batch, num_attention_heads, seqlen, head_dim)
        '''
        batch, num_key_value_heads, slen, head_dim = hidden_states.shape
        if n_rep == 1:
            return hidden_states
        hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads,
                                                        n_rep, slen, head_dim)
        return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen,head_dim)

# ffn层 class DbrxFFN(nn.Module)

x -> router -> experts

# router 层 class DbrxRouter(nn.Module)

x 先添加抖动(Jitter),默认关闭，如果 moe_jitter_eps 不为空，在训练时给 x 乘上一个随机噪声, 增加随机性, 避免专家塌缩

再由线性层分类, 4选1专家(默认topk=1)

# Moe层 class DbrxExperts(nn.Module) class DbrxExpertGLU(nn.Module)

silu激活函数，门结构(GLU Gated Linear Unit)

(act( w1 @ x ) * ( v1 @ x ) ) @ w2

---------------------------------------------------------------------------------------------------------------

## gemma

# 模型主体 class GemmaModel(GemmaPreTrainedModel)

x -> 28层Decoder -> RMSNorm

# Decoder层 class GemmaDecoderLayer(nn.Module)
pre norm
x 残差 -> RMSNorm -> attention -> 残差链接 -> x, 残差 -> MLP层 -> 残差链接

# attention层 class GemmaAttention(nn.Module)

RoPE, 可使用group-query head, 默认使用传统attention, q kv都是16个头

# 前馈层 class GemmaMLP(nn.Module)

gelu, GLU门结构, size从 3072 -> 24576 -> 3072

    class GemmaMLP(nn.Module):
        def __init__(self, config):
            super().__init__()
            self.config = config
            self.hidden_size = config.hidden_size
            self.intermediate_size = config.intermediate_size
            self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
            self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
            self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)
            self.act_fn = ACT2FN[config.hidden_act]

        def forward(self, x):
            return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))

-------------------------------------------------------------------------------------------

## DeepSeek v3

# 模型主体 class DeepseekV3Model(DeepseekV3PreTrainedModel):

x -> Embedding -> 61层Decoder -> RMSNorm

# Decoder块 class DeepseekV3DecoderLayer(nn.Module)

串行

x, 残差 -> RMSNorm -> attn -> 残差连接 -> x, 残差 -> RMSNorm -> MOE -> 残差连接

# Attention层 class DeepseekV3Attention(nn.Module)

MLA

主要分为两部分，一是LoRA思想，在生成Q,K,V时用中间层降维再升维；二是切分Q, K，部分采用RoPE变换，部分不采用RoPE变换

以下按照默认参数逐步进行

输入的特征维度 hidden_size = 7168

多头数 num_heads = 128

Q, KV 在LoRA时的中间维度 q_lora_rank = 1536, kv_lora_rank = 512

Q,K 每个头参与RoPE的维度大小 qk_rope_head_dim = 64, 不参与RoPE的维度大小qk_nope_head_dim = 128

V 每个头的维度大小 v_head_dim = 128

所以 q, k每个头的维度 q_head_dim = 64 + 128 = 192



*   ***q 的计算流程***

        self.q_a_proj = nn.Linear(self.hidden_size, config.q_lora_rank, bias=config.attention_bias) #w.shape = 7168,1536
        self.q_a_layernorm = DeepseekV3RMSNorm(config.q_lora_rank)
        self.q_b_proj = nn.Linear(config.q_lora_rank, self.num_heads * self.q_head_dim, bias=False) #w.shape = 1536,128*192

        q = self.q_b_proj(self.q_a_layernorm(self.q_a_proj(hidden_states)))

hidden_state 先经过内层的线性层，从 7168 降到1536，过RMSNorm，再经外层的线性层升到128 * 192, 最后一维拆成 [头数，q-NoPE + q-RoPE]

最后拆成两部分，一部分是特征向量维度 128 不参与RoPE， 一部分 64 参与RoPE

    q = q.view(bsz, q_len, self.num_heads, self.q_head_dim).transpose(1, 2) #注意形状
    q_nope, q_pe = torch.split(
        q, [self.qk_nope_head_dim, self.qk_rope_head_dim], dim=-1
    ) #切分成两个张量，形状分别为(B,nh(128),L,qk_nope_head_dim(128)), (B,nh(128),L,qk_rope_head_dim(64))

**最后我们在这一步得到了:**

**q_nope 形状是[B, nh(128), L, q_nope_head_dim(128)]**

**q_pe 形状是[B, nh(128), L, q_nope_head_dim(64)]**

*   ***k 和 v 的计算过程***


    self.kv_a_proj_with_mqa = nn.Linear(
        self.hidden_size,
        config.kv_lora_rank + config.qk_rope_head_dim,
        bias=config.attention_bias,
    ) #w.shape = 7168,512+64

    self.kv_a_layernorm = DeepseekV3RMSNorm(config.kv_lora_rank)

    self.kv_b_proj = nn.Linear(
        config.kv_lora_rank,
        self.num_heads
        * (self.q_head_dim - self.qk_rope_head_dim + self.v_head_dim),
        bias=False,
    ) #w.shape = 512,128*(192-64+128) = 512,32768


k 先LoRA降秩到 512 + 64， **拆成两部分** compressed_kv 512 和 k_pe 64， **注意这里和q的处理流程不同**

compressed_kv 这部分先RMSNorm，再升秩到 128 * 256， 最后一维拆成 [头数, k-NoPE + v] ，最后切分成 k 不参与RoPE的部分， 和v **这部分和q处理方式相同，hidden_state在这里同时用LoRA生成 k-NoPE 和 v**

    kv = (
        self.kv_b_proj(self.kv_a_layernorm(compressed_kv)) #(B,L,32768)
        .view(bsz, q_len, self.num_heads, self.qk_nope_head_dim + self.v_head_dim) #shape:B,L,128,256
        .transpose(1, 2) #shape:B,128,L,256
    )

    k_nope, value_states = torch.split(
        kv, [self.qk_nope_head_dim, self.v_head_dim], dim=-1
    ) #切分张量(B,nh(128),L,128), (B,nh(128),L,128)

剩下的 k_pe 先加一个头数的维度， 再转置

    k_pe = k_pe.view(bsz, q_len, 1, self.qk_rope_head_dim).transpose(1, 2)

**最后我们在这一步得到了:**

**k_nope 形状是[B, nh(128), L, k_nope_head_dim(128)]**

**k_pe 形状是[B, 1, L, q_nope_head_dim(64)]**

**value_states 形状是[B, nh(128), L, k_nope_head_dim(128)]**



*   ***RoPE***

把 q_pe, k_pe 送入RoPE各自变换，得到维度不变的 q_pe, k_pe

*   ***连接 q 和 k 的两部分***

首先新建两个随机初始化，形状是[B, nh(128), L, 128 + 64]的矩阵

对于Q，前128分配给q_nope [B, nh(128), L, 128]， 后64分配给q_pe [B, nh(128), L, 64]

对于K， 前128分配给k_nope [B, nh(128), L, 128]， 后64分配给k_pe [B, nh(128), L, 64] **这里在头数的维度上广播扩展**

    query_states = k_pe.new_empty(bsz, self.num_heads, q_len, self.q_head_dim)
    query_states[:, :, :, : self.qk_nope_head_dim] = q_nope
    query_states[:, :, :, self.qk_nope_head_dim :] = q_pe

    key_states = k_pe.new_empty(bsz, self.num_heads, q_len, self.q_head_dim)
    key_states[:, :, :, : self.qk_nope_head_dim] = k_nope
    key_states[:, :, :, self.qk_nope_head_dim :] = k_pe



*   pad V

因为 V 的最后一维现在是128，需要pad 0 匹配 Q, K的尺寸 192

*   最后计算QKV

softmax(Q @ KT / sqrt(192)) @ V


"""
